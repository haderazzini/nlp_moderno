{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP  Morderno em Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook foi feito para aprendizagem de NLP seguindo o  passo a passo do Patrick Harrison, Director of AI Engineering @ S&P Global, disponível [aqui](https://github.com/skipgram/modern-nlp-in-python-2019/blob/master/notebooks/Modern_NLP_in_Python.ipynb). Este notebook é provido como está e não representa nenhuma garantia. Além disso todo credito deve ser dado ao Patrick Harrison e não a mim.\n",
    "Você pode ver as explicações dele da versão de 2016 no [Youtube](https://www.youtube.com/watch?v=6zm9NC9uRkk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tópicos:\n",
    "\n",
    "Este tutorial apresenta um pipeline de processamento de ciência de dados e linguagem natural de ponta a ponta, começando com dados brutos e executando a preparação, modelagem, visualização e análise dos dados. Vamos abordar os seguintes pontos:\n",
    "\n",
    "* Um tour pelo conjunto de dados\n",
    "* Introdução ao processamento de texto com spaCy\n",
    "* Modelagem automática de frases\n",
    "* Modelagem de tópicos com LDA\n",
    "* Visualizando modelos de tópicos com pyLDAvis\n",
    "* Modelos de vetores de palavras com word2vec\n",
    "* Visualizando o word2vec com t-SNE\n",
    "* Categorização de texto (classificação) com o modelo textcat do spaCy\n",
    "* Vetores de palavras contextuais com spaCy Pytorch Transformers\n",
    "* ... e podemos até aprender uma coisa ou duas sobre Python ao longo do caminho.\n",
    "\n",
    "Vamos começar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. O conjunto de dados Yelp\n",
    "\n",
    "O conjunto de dados Yelp é um conjunto de dados publicado pelo Yelp, serviço de análise de negócios, para fins acadêmicos e educacionais. É um ótimo conjunto de dados do Yelp para demonstrações de aprendizado de máquina e processamento de linguagem natural, porque é grande (mas não tão grande que você precisa de seu próprio data center para processá-lo), bem conectado e qualquer pessoa pode se relacionar com ele - é sobre comida, afinal!\n",
    "\n",
    "Nota: Se você deseja executar este notebook de forma interativa em sua máquina local, precisará fazer o download de sua própria cópia do conjunto de dados do Yelp. Se você estiver revisando uma cópia estática do notebook online, poderá pular esta etapa. Veja como obter o conjunto de dados:\n",
    "\n",
    "* Visite a página do conjunto de dados do Yelp [aqui](https://www.yelp.com/dataset)\n",
    "* Clique em \"Download Dataset\"\n",
    "* Leia, concorde e respeite os termos de uso do Yelp!\n",
    "* O conjunto de dados é baixado como um arquivo .tar; desarquivar\n",
    "* Coloque os arquivos do conjunto de dados não compactados (business.json etc.) em um diretório chamado yelp_dataset\n",
    "* Coloque o diretório yelp_dataset no diretório de dados na pasta do projeto Modern NLP in Python\n",
    "\n",
    "É isso aí! Você está pronto para rodar.\n",
    "\n",
    "A iteração atual do conjunto de dados do Yelp (a partir desta demonstração) consiste nos seguintes dados:\n",
    "\n",
    "\n",
    "* 209 mil negócios\n",
    "* 8 milhões de avaliações de usuários\n",
    "* 4,5GB comprimidos\n",
    "\n",
    "<font color='red'>ATENÇÃO: se fizer o download dos dados compactados terá que fazer o unzip algumas vezes, se atente para como você vai organizar as pastas pois será importante na hora de passar o caminho para carregar os aquivos</font>\n",
    "\n",
    "> atualizado 08/04/2020  \n",
    "\n",
    "Ao focar apenas nos restaurantes, existem aproximadamente 59 mil restaurantes com aproximadamente 4,2 milhões de comentários de usuários escritos sobre eles.\n",
    "\n",
    "Os dados são fornecidos em um punhado de arquivos no formato .json. Usaremos os seguintes arquivos para nossa demonstração:\n",
    "\n",
    "* **business.json** - os registros para empresas individuais\n",
    "\n",
    "* **review.json** - os registros para comentários que os usuários escreveram sobre empresas\n",
    "\n",
    "Os arquivos são arquivos de texto (UTF-8) com um objeto json por linha, cada um correspondendo a um registro de dados individual. Vamos dar uma olhada em alguns exemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"business_id\":\"f9NumwFMBDn751xgFiRbNA\",\"name\":\"The Range At Lake Norman\",\"address\":\"10913 Bailey Rd\",\"city\":\"Cornelius\",\"state\":\"NC\",\"postal_code\":\"28031\",\"latitude\":35.4627242,\"longitude\":-80.8526119,\"stars\":3.5,\"review_count\":36,\"is_open\":1,\"attributes\":{\"BusinessAcceptsCreditCards\":\"True\",\"BikeParking\":\"True\",\"GoodForKids\":\"False\",\"BusinessParking\":\"{'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}\",\"ByAppointmentOnly\":\"False\",\"RestaurantsPriceRange2\":\"3\"},\"categories\":\"Active Life, Gun\\/Rifle Ranges, Guns & Ammo, Shopping\",\"hours\":{\"Monday\":\"10:0-18:0\",\"Tuesday\":\"11:0-20:0\",\"Wednesday\":\"10:0-18:0\",\"Thursday\":\"11:0-20:0\",\"Friday\":\"11:0-20:0\",\"Saturday\":\"11:0-20:0\",\"Sunday\":\"13:0-18:0\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#Lembre-se que o caminho onde você salvou o dataset pode ser diferente do meu, \n",
    "#então ajuste o comando abaixo de acordo coma  estrutura das suas pastas\n",
    "data_directory = os.path.join('yelp_dataset','_yelp_dataset')\n",
    "\n",
    "businesses_filepath = os.path.join(data_directory,'yelp_academic_dataset_business.json')\n",
    "\n",
    "with open(businesses_filepath) as f:\n",
    "    first_business_record = f.readline() \n",
    "\n",
    "print(first_business_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os registros comerciais consistem em pares de *chave-valor* que contêm informações sobre os negócios específicos. Alguns atributos em que estaremos interessados nesta demonstração incluem:\n",
    "\n",
    "* **business_id** - identificador exclusivo para empresas\n",
    "* **categories** - uma lista delimitada por vírgula que contém os valores de categoria relevantes das empresas\n",
    "\n",
    "O atributo de categorias é de interesse especial. Esta demonstração será focada em restaurantes, indicados pela presença da tag Restaurantes na lista de categorias. Além disso, a lista de categorias pode conter informações mais detalhadas sobre restaurantes, como o tipo de comida que eles servem.\n",
    "\n",
    "Os registros de revisão são armazenados de maneira semelhante - pares de chave e valor contendo informações sobre as revisões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"xQY8N_XvtGbearJ5X4QryQ\",\"user_id\":\"OwjRMXRC0KyPrIlcjaXeFQ\",\"business_id\":\"-MhfebM0QIsKt87iDN-FNw\",\"stars\":2.0,\"useful\":5,\"funny\":0,\"cool\":0,\"text\":\"As someone who has worked with many museums, I was eager to visit this gallery on my most recent trip to Las Vegas. When I saw they would be showing infamous eggs of the House of Faberge from the Virginia Museum of Fine Arts (VMFA), I knew I had to go!\\n\\nTucked away near the gelateria and the garden, the Gallery is pretty much hidden from view. It's what real estate agents would call \\\"cozy\\\" or \\\"charming\\\" - basically any euphemism for small.\\n\\nThat being said, you can still see wonderful art at a gallery of any size, so why the two *s you ask? Let me tell you:\\n\\n* pricing for this, while relatively inexpensive for a Las Vegas attraction, is completely over the top. For the space and the amount of art you can fit in there, it is a bit much.\\n* it's not kid friendly at all. Seriously, don't bring them.\\n* the security is not trained properly for the show. When the curating and design teams collaborate for exhibitions, there is a definite flow. That means visitors should view the art in a certain sequence, whether it be by historical period or cultural significance (this is how audio guides are usually developed). When I arrived in the gallery I could not tell where to start, and security was certainly not helpful. I was told to \\\"just look around\\\" and \\\"do whatever.\\\" \\n\\nAt such a *fine* institution, I find the lack of knowledge and respect for the art appalling.\",\"date\":\"2015-04-15 05:21:16\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_json_filepath = os.path.join(data_directory, 'yelp_academic_dataset_review.json')\n",
    "\n",
    "with open(review_json_filepath) as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print(first_review_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns atributos a serem observados nos registros de revisão:\n",
    "\n",
    "* **business_id** - indica de que empresa trata a revisão\n",
    "* **texto** - o texto que o usuário escreveu\n",
    "\n",
    "O atributo text será nosso foco hoje!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Preparando os dados\n",
    "\n",
    "json é um formato de arquivo útil para troca de dados, mas normalmente não é o mais utilizável para qualquer tipo de trabalho de modelagem. Vamos fazer um pouco mais de preparação de dados para obter nossos dados em um formato mais utilizável. Nosso próximo bloco de código fará o seguinte:\n",
    "\n",
    "1. Leia em cada registro comercial e converta-o em um dicionário Python\n",
    "1. Filtre os registros comerciais que não são sobre restaurantes (ou seja, não estão na categoria \"Restaurante\")\n",
    "1. Crie um conjunto completo de IDs comerciais para restaurantes, que usaremos na próxima etapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63,944 restaurantes no dataset.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "restaurant_ids = set()\n",
    "\n",
    "# abra o arquivo de negócios\n",
    "with open(businesses_filepath,  encoding='utf-8') as f:\n",
    "    \n",
    "    # faça a ineracao sobre cada linha\n",
    "    for business_json in f:\n",
    "        \n",
    "        # converta cada registro em json  para um dict do Python\n",
    "        business = json.loads(business_json)\n",
    "        \n",
    "        # Se não existir o atributo 'categories' passe para o proximo,\n",
    "        if not business.get('categories'):\n",
    "            continue\n",
    "        \n",
    "        # Se o negócio não for restaurante passe para o proximo,\n",
    "        if 'Restaurants' not in business['categories']:\n",
    "            continue\n",
    "            \n",
    "        # adicione a id do restaurante ao conjunto restaurant_ids\n",
    "        restaurant_ids.add(business['business_id'])\n",
    "\n",
    "# transforme restaurant_ids em frozenset, uma vez que não temos quer altera-lo mais\n",
    "restaurant_ids = frozenset(restaurant_ids)\n",
    "\n",
    "# o numero de restaurantes\n",
    "print(f'{len(restaurant_ids):,} restaurantes no dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, criaremos um novo arquivo que contenha apenas o texto das avaliações sobre restaurantes, com uma avaliação por linha no arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_directory = os.path.join('scratch')\n",
    "\n",
    "# cria o diretorio scratch directory se ele não existir\n",
    "try:\n",
    "    os.mkdir(scratch_directory)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "review_txt_filepath = os.path.join(scratch_directory, 'review_text_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um total de 5,058,161 avaliação de restaurantes escritas arquivo txt.\n"
     ]
    }
   ],
   "source": [
    "# Isso leva um bom tempo executando. Se quiser fazer isso da primeira vez coloque execute = True\n",
    "\n",
    "execute = False\n",
    "\n",
    "if execute:\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # cria e abre o novo arquivo no modo escrita\n",
    "    with open(review_txt_filepath, 'w', encoding='utf-8') as review_txt_file:\n",
    "\n",
    "        # abre o arquivos de avaliação json\n",
    "        with open(review_json_filepath,  encoding='utf-8') as review_json_file:\n",
    "\n",
    "            # faz o loop através das avaliação e converte em dicionário\n",
    "            for review_json in review_json_file:\n",
    "                review = json.loads(review_json, encoding='utf-8')\n",
    "\n",
    "                # se a avaliação não for sobre restaurante passe para o próximo\n",
    "                if review['business_id'] not in restaurant_ids:\n",
    "                    continue\n",
    "\n",
    "                # escreve a avaliação do restaurante no arquivo como uma linha\n",
    "                # adiciona uma new line além do existente na avaliação\n",
    "                review_txt_file.write(review['text'].replace('\\n', '\\\\n') + '\\n')\n",
    "                review_count += 1\n",
    "\n",
    "    print(f'Um total de {review_count:,} avaliação de restaurantes escritas arquivo txt.')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # Apenas conta o número de avaliação que existe no arquivo txt\n",
    "    with open(review_txt_filepath, encoding='utf-8') as review_txt_file:\n",
    "        for review_count, line in enumerate(review_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print(f'Um total de {review_count:,} avaliação de restaurantes escritas arquivo txt.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. spaCy —  NLP nível Industrial em Python\n",
    "\n",
    "spaCy é uma biblioteca de processamento de linguagem natural -  **Natural Language Processing (NLP)** de nível industrial para Python. O objetivo da spaCy é retirar os recentes avanços no processamento de linguagem natural dos documentos de pesquisa e colocá-los nas mãos dos usuários para criar software de produção.\n",
    "\n",
    "O spaCy lida com muitas tarefas comumente associadas à construção de um pipeline de processamento de linguagem natural de ponta a ponta:\n",
    "\n",
    "* Tokenização\n",
    "* Normalização de texto, como minúsculas, lematização e análise de forma de token\n",
    "* Marcação de parte da fala\n",
    "* Análise de dependência sintática\n",
    "* Detecção de limite de sentença\n",
    "* Reconhecimento e anotação de entidade nomeada\n",
    "\n",
    "Seguindo a tradição \"pilhas inclusas\" do Python, o spaCy contém dados e modelos internos que você pode usar imediatamente para processar texto de idioma inglês de uso geral:\n",
    "\n",
    "* Grande vocabulário em inglês, incluindo listas de palavras irrelevantes\n",
    "* \"Probabilidades\" de Tokens\n",
    "* Vetores de palavras\n",
    "\n",
    "<font color='green'> Nota do Tradutor: O spaCy também tem o Português, entretanto os modelos não são tão avançados quanto para o Inglês. </font>\n",
    "\n",
    "spaCy é escrito em Cython otimizado, o que significa que é rápido. De acordo com algumas fontes independentes, é o analisador sintático mais rápido disponível em qualquer idioma. As principais partes do pipeline de análise spaCy são escritas em C puro, permitindo multithreading eficiente (ou seja, spaCy pode liberar o GIL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Atenção!: Se você ainda não tiver instalado execute as linhas abaixo, caso contrário apenas ignore </font>. Para transforma a célula em código novamente, selecione a celula depois click no menu Cell> Cell Type> Code, ou selecione a celula e aperte Y."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Inciando o Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#nlp = spacy.load('en_core_web_md') No tutorial do Harrison era esse mas parece que foi alterado no spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos pegar uma amostra de avaliação para brincar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped in here for an after work drink and some ribs. First impression when I walk in is the place is dirty. Patio is niceand modern but somebody should be cleaning the tables better.\n",
      "I had the Rib and Wing combo which was ok. I cannot complain but I cannot tell you that I was impressed either. It was supposed to be the special of the day but it just looked like they cut the price and the portion size. Fries were good though!\n",
      "The key is at this restaurant is the service, or should i say the lack of service. Both bartender and waitress never smile, are not very attentive and do not seem to care about being there. This is probably a result of the management/ownership lack of attentiveness. I saw the owner and he looked more like an overseer then someone who was pushing his team to succeed. \n",
      "Stay away, until the ownership understands what it means to be in the hospitality business.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_num = 754601\n",
    "# atenção este numero pode trazer uma avaliação diferente da apresenta aqui, \n",
    "# aconteceu essa diferença entre o tutorial do Harrison e este\n",
    "\n",
    "with open(review_txt_filepath, encoding='utf-8') as f:\n",
    "    sample_review = list(it.islice(f, review_num, review_num+1))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "        \n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Passe a avaliação para o spaCy, e se prepare para esperar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 66.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "... uma fração de segundo. Vamos ver o que ele estava fazendo durante este tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped in here for an after work drink and some ribs. First impression when I walk in is the place is dirty. Patio is niceand modern but somebody should be cleaning the tables better.\n",
      "I had the Rib and Wing combo which was ok. I cannot complain but I cannot tell you that I was impressed either. It was supposed to be the special of the day but it just looked like they cut the price and the portion size. Fries were good though!\n",
      "The key is at this restaurant is the service, or should i say the lack of service. Both bartender and waitress never smile, are not very attentive and do not seem to care about being there. This is probably a result of the management/ownership lack of attentiveness. I saw the owner and he looked more like an overseer then someone who was pushing his team to succeed. \n",
      "Stay away, until the ownership understands what it means to be in the hospitality business.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parsed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Parece a mesma coisa! O que aconteceu sob o capô?\n",
    "\n",
    "E quanto à detecção e segmentação de sentenças?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "Stopped in here for an after work drink and some ribs.\n",
      "\n",
      "Sentence 2:\n",
      "First impression when I walk in is the place is dirty.\n",
      "\n",
      "Sentence 3:\n",
      "Patio is niceand modern but somebody should be cleaning the tables better.\n",
      "\n",
      "\n",
      "Sentence 4:\n",
      "I had the Rib and Wing combo which was ok.\n",
      "\n",
      "Sentence 5:\n",
      "I cannot complain\n",
      "\n",
      "Sentence 6:\n",
      "but I cannot tell you that I was impressed either.\n",
      "\n",
      "Sentence 7:\n",
      "It was supposed to be the special of the day\n",
      "\n",
      "Sentence 8:\n",
      "but it just looked like they cut the price and the portion size.\n",
      "\n",
      "Sentence 9:\n",
      "Fries were good though!\n",
      "\n",
      "\n",
      "Sentence 10:\n",
      "The key is at this restaurant is the service, or should i say the lack of service.\n",
      "\n",
      "Sentence 11:\n",
      "Both bartender and waitress never smile, are not very attentive and do not seem to care about being there.\n",
      "\n",
      "Sentence 12:\n",
      "This is probably a result of the management/ownership lack of attentiveness.\n",
      "\n",
      "Sentence 13:\n",
      "I saw the owner and he looked more like an overseer then someone who was pushing his team to succeed. \n",
      "\n",
      "\n",
      "Sentence 14:\n",
      "Stay away, until the ownership understands what it means to be in the hospitality business.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print(f'Sentence {num + 1}:')\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "E a normalização de texto, como lematização e análise de forma de token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Stopped</td>\n",
       "      <td>stop</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>here</td>\n",
       "      <td>here</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>hospitality</td>\n",
       "      <td>hospitality</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>business</td>\n",
       "      <td>business</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_text  token_lemma token_shape\n",
       "0        Stopped         stop       Xxxxx\n",
       "1             in           in          xx\n",
       "2           here         here        xxxx\n",
       "3            for          for         xxx\n",
       "4             an           an          xx\n",
       "..           ...          ...         ...\n",
       "184          the          the         xxx\n",
       "185  hospitality  hospitality        xxxx\n",
       "186     business     business        xxxx\n",
       "187            .            .           .\n",
       "188           \\n           \\n          \\n\n",
       "\n",
       "[189 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(\n",
    "    zip(token_text, token_lemma, token_shape),\n",
    "    columns=['token_text', 'token_lemma', 'token_shape']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "E sobre *part of speech tagging?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Stopped</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>here</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>an</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>hospitality</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>business</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_text part_of_speech\n",
       "0        Stopped           VERB\n",
       "1             in            ADV\n",
       "2           here            ADV\n",
       "3            for            ADP\n",
       "4             an            DET\n",
       "..           ...            ...\n",
       "184          the            DET\n",
       "185  hospitality           NOUN\n",
       "186     business           NOUN\n",
       "187            .          PUNCT\n",
       "188           \\n          SPACE\n",
       "\n",
       "[189 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(\n",
    "    zip(token_text, token_pos),\n",
    "    columns=['token_text', 'part_of_speech']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "E a detecção de entidades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Stopped in here for an after work drink and some ribs. \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    First\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " impression when I walk in is the place is dirty. Patio is niceand modern but somebody should be cleaning the tables better.</br>I had the \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rib\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " and Wing combo which was ok. I cannot complain but I cannot tell you that I was impressed either. It was supposed to be the special of \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the day\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " but it just looked like they cut the price and the portion size. Fries were good though!\n",
       "The key is at this restaurant is the service, or should i say the lack of service. Both bartender and waitress never smile, are not very attentive and do not seem to care about being there. This is probably a result of the management/ownership lack of attentiveness. I saw the owner and he looked more like an overseer then someone who was pushing his team to succeed. \n",
       "Stay away, until the ownership understands what it means to be in the hospitality business.\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(parsed_review, style=\"ent\",jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: First - ORDINAL\n",
      "\n",
      "Entity 2: Rib - LOC\n",
      "\n",
      "Entity 3: the day - DATE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print(f'Entity {num + 1}:', entity, '-', entity.label_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'> Nota do Tradutor: Parece que nem tudo é perfeito no mundo. O spaCy reconheceu \"Rib\" como uma localização sendo que é um prato \"costela\"</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "E a análise de entidade no nível de token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Stopped</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>here</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>an</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>hospitality</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>business</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token_text entity_type inside_outside_begin\n",
       "0        Stopped                                O\n",
       "1             in                                O\n",
       "2           here                                O\n",
       "3            for                                O\n",
       "4             an                                O\n",
       "..           ...         ...                  ...\n",
       "184          the                                O\n",
       "185  hospitality                                O\n",
       "186     business                                O\n",
       "187            .                                O\n",
       "188           \\n                                O\n",
       "\n",
       "[189 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(\n",
    "    zip(token_text, token_entity_type, token_entity_iob),\n",
    "    columns=['token_text', 'entity_type', 'inside_outside_begin']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "E uma variedade de outros atributos no nível de token, como a frequência relativa de tokens, e se um token corresponde ou não a alguma dessas categorias?\n",
    "\n",
    "* stopword\n",
    "* pontuação\n",
    "* espaço em branco\n",
    "* representa um número\n",
    "* se o token está ou não incluído no vocabulário padrão do spaCy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Stopped</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>here</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>for</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>an</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>the</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>hospitality</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>business</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>.</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>\\n</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            text  log_probability stop? punctuation? whitespace? number?  \\\n",
       "0        Stopped            -20.0                                          \n",
       "1             in            -20.0   Yes                                    \n",
       "2           here            -20.0   Yes                                    \n",
       "3            for            -20.0   Yes                                    \n",
       "4             an            -20.0   Yes                                    \n",
       "..           ...              ...   ...          ...         ...     ...   \n",
       "184          the            -20.0   Yes                                    \n",
       "185  hospitality            -20.0                                          \n",
       "186     business            -20.0                                          \n",
       "187            .            -20.0                Yes                       \n",
       "188           \\n            -20.0                            Yes           \n",
       "\n",
       "    out of vocab.?  \n",
       "0              Yes  \n",
       "1              Yes  \n",
       "2              Yes  \n",
       "3              Yes  \n",
       "4              Yes  \n",
       "..             ...  \n",
       "184            Yes  \n",
       "185            Yes  \n",
       "186            Yes  \n",
       "187            Yes  \n",
       "188            Yes  \n",
       "\n",
       "[189 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [\n",
    "        (\n",
    "        token.orth_,\n",
    "        token.prob,\n",
    "        token.is_stop,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.like_num,\n",
    "        token.is_oov\n",
    "        ) for token in parsed_review\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    token_attributes,\n",
    "    columns=[\n",
    "        'text',\n",
    "        'log_probability',\n",
    "        'stop?',\n",
    "        'punctuation?',\n",
    "        'whitespace?',\n",
    "        'number?',\n",
    "        'out of vocab.?'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (\n",
    "    df\n",
    "    .loc[:, 'stop?':'out of vocab.?']\n",
    "    .applymap(lambda x: 'Yes' if x else '')\n",
    "    )\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se o texto que você processar é Inglês de uso geral (i.e., não é de um domínio específico, como literatura médica), O spaCy está pronto para o uso imediato.\n",
    "\n",
    "Penso que acabará por se tornar uma parte essencial do ecossistema de ciência de dados Python - fará pela computação em linguagem natural o que outras grandes bibliotecas fizeram para a computação numérica.\n",
    "\n",
    "# Phrase Modeling \n",
    "\n",
    "A *Phrase Modeling* é outra abordagem para aprender combinações de tokens que juntos representam conceitos significativos de várias palavras. Podemos desenvolver *phrase model* repetindo as palavras em nossas análises e procurando por palavras que co-ocorram (ou seja, aparecem uma após a outra) juntas com muito mais frequência do que você esperaria que elas sugissem por acaso. A fórmula para nossos *phrase models* que usaremos para determinar se dois token $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "\n",
    "\n",
    "$$\\frac{count(A\\ B) - count_{min}}{count(A) * count(B)} * N > threshold$$\n",
    "\n",
    "...where:\n",
    "\n",
    "* $count(A)$ é o número de vezes que o token $A$ aparece no corpus\n",
    "* $count(B)$ é o número de vezes que o token $B$ aparece no corpus\n",
    "* $count(A\\ B)$ é o número de vezes que os tokens $A\\ B$ aparecem no corpus na ordem exata\n",
    "* $N$ é o tamanho total do vocabulário do corpus\n",
    "* $count_{min}$  é um parâmetro definido pelo usuário para asegurar que a frase aceita ocorre ao menos uma quantidade mínima de vezes\n",
    "* $threshold$ é um parâmetro definido pelo usuário para controlar a intensidade que deve ter a relação entre os dois tokes para que seja aceita como um *phrase model*\n",
    "\n",
    "\n",
    "Uma vez que nosso modelo de frase foi treinado em nosso corpus, podemos aplicá-lo ao novo texto. Quando nosso modelo encontra dois tokens no novo texto que é identificado como uma frase, ele mescla os dois em um único novo token.\n",
    "\n",
    "A *phrase modeling* é superficialmente semelhante à detecção de entidades nomeadas, na medida em que você esperaria que as entidades nomeadas se tornassem frases no modelo (para que new york se tornasse new_york). Mas você também espera que expressões com várias palavras que representem conceitos comuns, mas que não sejam entidades nomeadas especificamente (como happy hour), também se tornem frases no modelo.\n",
    "\n",
    "Nós nos voltamos para a indispensável biblioteca [gensim](https://radimrehurek.com/gensim/index.html) para nos ajudar na modelagem de frases - a classe [Phrases](https://radimrehurek.com/gensim/models/phrases.html) em particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Atenção!: Se você ainda não tiver instalado execute as linhas abaixo, caso contrário apenas ignore </font>. Para transforma a célula em código novamente, selecione a celula depois click no menu Cell> Cell Type> Code, ou selecione a celula e aperte Y."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enquanto realizamos a modelagem de frases, faremos algumas transformações de dados iterativas ao mesmo tempo. Nosso roteiro para a preparação de dados inclui:\n",
    "\n",
    "* Segmente o texto de avaliações completas em frases e normalize o texto\n",
    "* Modelagem de frase de primeira ordem $ \\rightarrow $ aplica o modelo de frase de primeira ordem para transformar sentenças\n",
    "* Modelagem de frase de segunda ordem $ \\rightarrow $ aplica o modelo de frase de segunda ordem para transformar sentenças\n",
    "* Aplicar normalização de texto e modelo de frase de segunda ordem ao texto de revisões completas\n",
    "* Usaremos esses dados transformados como entrada para algumas abordagens de modelagem de nível superior nas seções a seguir.\n",
    "\n",
    "Primeiro, vamos definir algumas funções auxiliares que usaremos para normalização de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    funcao de auxilio para eliminar tokens\n",
    "    que sao so pontuacao e espaco em branco\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def pronoun_lemmatize(token):\n",
    "    \"\"\"\n",
    "    funcao de auxilio para preservar pronomes e tansforma em minuscula enquanto faz lematizacao\n",
    "    \"\"\"\n",
    "    \n",
    "    if token.lemma_ == '-PRON-':\n",
    "        return token.lower_\n",
    "    \n",
    "    else:\n",
    "        return token.lemma_.lower()\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    funcao geradora para ler avaliacoes do arquivo \n",
    "    e desmarque as quebras de linha originais no texto\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename,encoding='utf-8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida, usaremos o spaCy para:\n",
    "\n",
    "* Fazer iterações sobre as avaliações no arquivo review_txt_all.txt que criamos antes\n",
    "* Segmentar as revisões em frases individuais\n",
    "* Remover pontuação e excesso de espaço em branco\n",
    "* Lematização do texto\n",
    "\n",
    "... e faça isso com o benefício do multiprocessamento, graças à função nlp.pipe() do spaCy. Escreveremos esses dados novamente em um novo arquivo (sentence_lemmatized_all), com uma sentença normalizada por linha. Durante o processo, também pré-processaremos o texto das avaliações completas, não segmentadas por sentenças, da mesma maneira e salvaremos em um arquivo chamado review_lemmatized_all.\n",
    "\n",
    "Usaremos todos esses dados posteriormente para aprendizagem dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lemmatized_filepath = os.path.join(scratch_directory, 'review_lemmatized_all.txt')\n",
    "sentence_lemmatized_filepath = os.path.join(scratch_directory, 'sentence_lemmatized_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **<font color='red'>Atenção: se você deseja executar novamente o pré-processamento de texto, a célula seguinte demorou cerca de 18 horas para executar o sentenciamento e lematização de todo o texto de revisão de restaurante no conjunto de dados do Yelp.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isso vai demorar um bom tempo- coloque execute = True\n",
    "# se você quiser executar estes dados por você mesmo\n",
    "\n",
    "execute = False\n",
    "\n",
    "if execute:\n",
    "\n",
    "    with open(review_lemmatized_filepath, 'w',encoding='utf-8') as review_file:\n",
    "        with open(sentence_lemmatized_filepath, 'w',encoding='utf-8') as sentence_file:\n",
    "            \n",
    "            pipe = nlp.pipe(\n",
    "                line_review(review_txt_filepath),\n",
    "                batch_size=10000,\n",
    "                n_threads=8\n",
    "                )\n",
    "            \n",
    "            for parsed_review in pipe:\n",
    "                \n",
    "                # Lematização dos textos das avaliações, removendo pontuação e espaços em branco\n",
    "                lemmatized_review = ' '.join([\n",
    "                    pronoun_lemmatize(token)\n",
    "                    for token in parsed_review\n",
    "                    if not punct_space(token)\n",
    "                    ])\n",
    "                \n",
    "                # salva o texto de cada avaliação lematixada como uma nova linha no arquivo\n",
    "                review_file.write(lemmatized_review + '\\n')\n",
    "        \n",
    "                # Faz iteração sobre cada sentença da avaliação\n",
    "                for sent in parsed_review.sents:\n",
    "                    \n",
    "                    # Lematização do texto de cada sentença\n",
    "                    lemmatized_sentence = ' '.join([\n",
    "                        pronoun_lemmatize(token)\n",
    "                        for token in sent\n",
    "                        if not punct_space(token)\n",
    "                        ])\n",
    "                    \n",
    "                    # salva o texto de cada lemtização como uma linha no arquivo\n",
    "                    sentence_file.write(lemmatized_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se seus dados estão organizados como o nosso arquivo sentença_lemmatized_all agora é - um grande arquivo de texto com um documento / sentença por linha - a classe LineSentence do gensim fornece um iterador conveniente para trabalhar com outros componentes do gensim. Ele transmite os documentos / frases do disco, para que você nunca precise reter o corpus inteiro na RAM de uma só vez. Isso permite que você dimensione seu pipeline de modelagem para corpora potencialmente muito grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_unigrams = LineSentence(sentence_lemmatized_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos dar uma olhada em algumas frases de exemplo em nosso novo arquivo transformado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we also order one sushi roll because we have to try at least one\n",
      "\n",
      "and we love it\n",
      "\n",
      "definitely will be come back to try more thing\n",
      "\n",
      "place be really small and there be only one server but we come when it be really slow and the service be pretty fast\n",
      "\n",
      "all in all very happy we get to try this place out\n",
      "\n",
      "10 p.m. on a super bowl sunday\n",
      "\n",
      "and they be already close\n",
      "\n",
      "weak no wonder the hard rock be die off\n",
      "\n",
      "a close friend be in town and so instead of take him to a more well establish joint we decide to try the newly open choolah\n",
      "\n",
      "we be not disappointed\n",
      "\n",
      "i be a bit of an amateur chef myself and consider my palate to be fairly sophisticated when it come to all kind of south asian cuisine\n",
      "\n",
      "this be not authentic indian food we do not have rice and salad bowl but it be good wholesome high quality indian food\n",
      "\n",
      "i order a bowl of rice yellow daal and the koftas meatball\n",
      "\n",
      "the daal be perfect the way daal should be cook\n",
      "\n",
      "the koftas be a little bland for my taste but otherwise not bad\n",
      "\n",
      "soft and the right texture\n",
      "\n",
      "pair with the many sauce and onion and the whole dish be wholesome and rich and very filling\n",
      "\n",
      "my friend order the biryani which while not bad still be not the way real biryani should be and i come from a city know for its biryani\n",
      "\n",
      "but then i still have not find a single place in america that make biryani the way they do it back home\n",
      "\n",
      "the ambiance be good and there be plenty of room to sit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence_unigrams in it.islice(sentences_unigrams, 80, 100):\n",
    "    print(' '.join(sentence_unigrams))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Em seguida, aprenderemos um modelo de frase que vinculará palavras individuais a frases de duas palavras. Esperamos que palavras que juntas representem um conceito específico, como \"pin ball\", sejam vinculadas para formar um novo token único: \"pin_ball\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(scratch_directory, 'bigram_phrase_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **<font color='red'>Atenção: se você deseja executar o processamento de texto, a célula seguinte demorou cerca de 12 minutos  para executar.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isso leva um bom tempo executando - set execute = True\n",
    "# se você quiser executar por você mesmo.\n",
    "\n",
    "execute = False\n",
    "\n",
    "if execute:\n",
    "\n",
    "    bigram_phrases = Phrases(sentences_unigrams)\n",
    "    \n",
    "    # Transforma as frases em objetos \"Phraser\",\n",
    "    # o que é otimizado para acelerar a memória\n",
    "    bigram_phrases = Phraser(bigram_phrases)\n",
    "    bigram_phrases.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrega do disco o modelo finalizado\n",
    "bigram_phrases = Phraser.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos um modelo de frase treinado para pares de palavras, vamos aplicá-lo aos dados das frases de revisão e explorar os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_bigrams_filepath = os.path.join(scratch_directory, 'sentence_bigram_phrases_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **<font color='red'>Atenção: se você deseja executar o processamento de texto, a célula seguinte demorou cerca de 17 minutos  para executar.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isso leva um bom tempo executando - set execute = True\n",
    "# se você quiser executar por você mesmo.\n",
    "\n",
    "execute = True\n",
    "\n",
    "if execute:\n",
    "\n",
    "    with open(sentences_bigrams_filepath, 'w',encoding='utf-8') as f:\n",
    "        \n",
    "        for sentence_unigrams in sentences_unigrams:\n",
    "            \n",
    "            sentence_bigrams = ' '.join(bigram_phrases[sentence_unigrams])\n",
    "            \n",
    "            f.write(sentence_bigrams + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_bigrams = LineSentence(sentences_bigrams_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we also order one sushi roll because we have to try at least one\n",
      "\n",
      "and we love it\n",
      "\n",
      "definitely will be come back to try more thing\n",
      "\n",
      "place be really small and there be only one server but we come when it be really slow and the service be pretty fast\n",
      "\n",
      "all in all very happy we get to try this place out\n",
      "\n",
      "10 p.m. on a super bowl sunday\n",
      "\n",
      "and they be already close\n",
      "\n",
      "weak no wonder the hard rock be die off\n",
      "\n",
      "a close friend be in town and so instead of take him to a more well establish joint we decide to try the newly_open choolah\n",
      "\n",
      "we be not disappointed\n",
      "\n",
      "i be a bit of an amateur chef myself and consider my palate to be fairly sophisticated when it come to all kind of south asian cuisine\n",
      "\n",
      "this be not authentic indian food we do not have rice and salad bowl but it be good wholesome high quality indian food\n",
      "\n",
      "i order a bowl of rice yellow_daal and the koftas meatball\n",
      "\n",
      "the daal be perfect the way daal should be cook\n",
      "\n",
      "the koftas be a little bland for my taste but otherwise not bad\n",
      "\n",
      "soft and the right texture\n",
      "\n",
      "pair with the many sauce and onion and the whole dish be wholesome and rich and very filling\n",
      "\n",
      "my friend order the biryani which while not bad still be not the way real biryani should be and i come from a city know for its biryani\n",
      "\n",
      "but then i still have not find a single place in america that make biryani the way they do it back home\n",
      "\n",
      "the ambiance be good and there be plenty of room to sit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence_bigrams in it.islice(sentences_bigrams, 80, 100):\n",
    "    print(' '.join(sentence_bigrams))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que a frase modelagem funcionou! Agora vemos frases de duas palavras, como \"pin_ball\" e \"saturday_morning\", vinculadas no texto como um único token. Em seguida, treinaremos um modelo de frase de segunda ordem. Aplicaremos o modelo de frase de segunda ordem sobre os dados já transformados, para que combinações incompletas de palavras como \"ms_pac man\" sejam totalmente associadas a \"ms_pac_man\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(scratch_directory, 'trigram_phrase_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isso leva um bom tempo executando - set execute = True\n",
    "# se você quiser executar por você mesmo.\n",
    "\n",
    "execute = True\n",
    "\n",
    "if execute:\n",
    "\n",
    "    trigram_phrases = Phrases(sentences_bigrams)\n",
    "    \n",
    "    # Transforma as frases em objetos \"Phraser\",\n",
    "    # o que é otimizado para acelerar a memória\n",
    "    trigram_phrases = Phraser(trigram_phrases)\n",
    "    trigram_phrases.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrega do disco o modelo finalizado\n",
    "trigram_phrases = Phraser.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicaremos nosso modelo de frase de segunda ordem treinado em nossas frases transformadas de primeira ordem, escreveremos os resultados em um novo arquivo e exploraremos algumas das frases transformadas de segunda ordem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_trigrams_filepath = os.path.join(scratch_directory, 'sentence_trigram_phrases_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isso leva um bom tempo executando - set execute = True\n",
    "# se você quiser executar por você mesmo.\n",
    "\n",
    "execute = True\n",
    "\n",
    "if execute:\n",
    "\n",
    "    with open(sentences_trigrams_filepath, 'w',encoding='utf-8') as f:\n",
    "        \n",
    "        for sentence_bigrams in sentences_bigrams:\n",
    "            \n",
    "            sentence_trigrams = ' '.join(trigram_phrases[sentence_bigrams])\n",
    "            \n",
    "            f.write(sentence_trigrams + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_trigrams = LineSentence(sentences_trigrams_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence_trigrams in it.islice(sentences_trigrams, 60, 70):\n",
    "    print(' '.join(sentence_trigrams))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Parece que o modelo de frase de segunda ordem foi bem-sucedido. Agora estamos vendo frases de três palavras, como \"pin_ball_machine\" e \"ms_pac_man\".\n",
    "\n",
    "A etapa final do nosso processo de preparação de texto volta ao texto completo das revisões. Vamos executar o texto completo das revisões por meio de um pipeline que aplica nossa normalização de texto e modelos de frase.\n",
    "\n",
    "Além disso, removeremos as palavras-chave neste momento. Palavras de interrupção são palavras muito comuns, como a, the, e assim por diante, que desempenham funções funcionais na linguagem natural, mas geralmente não contribuem para o significado geral do texto. A filtragem de palavras irrelevantes é um procedimento comum que permite que as técnicas de modelagem de PNL de nível superior se concentrem nas palavras que carregam mais peso semântico.\n",
    "\n",
    "Por fim, escreveremos o texto transformado em um novo arquivo, com uma revisão por linha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_trigrams_filepath = os.path.join(scratch_directory, 'review_trigrams_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **<font color='red'>Atenção: se você deseja executar o processamento de texto, a célula seguinte demorou cerca de 30 minutos  para executar.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isso leva um bom tempo executando - set execute = True\n",
    "# se você quiser executar por você mesmo.\n",
    "\n",
    "execute = False\n",
    "\n",
    "if execute:\n",
    "    \n",
    "    reviews_lemmatized = LineSentence(review_lemmatized_filepath)\n",
    "\n",
    "    with open(review_trigrams_filepath, 'w') as f:\n",
    "        \n",
    "        for review_unigrams in reviews_lemmatized:\n",
    "                        \n",
    "            # apply the first-order and second-order phrase models\n",
    "            review_bigrams = bigram_phrases[review_unigrams]\n",
    "            review_trigrams = trigram_phrases[review_bigrams]\n",
    "\n",
    "            # remove any remaining stopwords\n",
    "            review_trigrams = [\n",
    "                term\n",
    "                for term in review_trigrams\n",
    "                if term not in nlp.Defaults.stop_words\n",
    "                ]\n",
    "\n",
    "            # write the transformed review as a line in the new file\n",
    "            review_trigrams = ' '.join(review_trigrams)\n",
    "            f.write(review_trigrams + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
